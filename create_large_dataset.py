#!/usr/bin/env python3
"""
åˆ›å»ºå¤§å‹è®­ç»ƒæ•°æ®é›†ï¼Œè§£å†³è®­ç»ƒæ•°æ®ä¸è¶³é—®é¢˜
"""

import os
import h5py
import numpy as np

def create_large_rb_dataset():
    """åˆ›å»ºå¤§å‹RBæ•°æ®é›†"""
    print("ğŸ”„ åˆ›å»ºå¤§å‹RBè®­ç»ƒæ•°æ®é›†")
    print("=" * 50)

    data_dir = './rb_data_numerical'
    os.makedirs(data_dir, exist_ok=True)

    output_file = os.path.join(data_dir, 'rb_data_Ra_1e+05.h5')

    # åˆ é™¤æ—§æ–‡ä»¶
    if os.path.exists(output_file):
        print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ–‡ä»¶: {output_file}")
        os.remove(output_file)

    # å‚æ•°è®¾ç½®
    Ra = 1e5
    Pr = 0.7
    Lx, Ly = 3.0, 1.0

    # é«˜åˆ†è¾¨ç‡ç½‘æ ¼
    nx, ny = 768, 192  # é«˜åˆ†è¾¨ç‡
    nt = 6000          # æ›´å¤šæ—¶é—´æ­¥

    print(f"ğŸ“Š æ•°æ®é›†å‚æ•°:")
    print(f"  åˆ†è¾¨ç‡: {nx} x {ny}")
    print(f"  æ—¶é—´æ­¥: {nt}")
    print(f"  Raæ•°: {Ra}")

    # åˆ›å»ºåæ ‡ç½‘æ ¼
    x = np.linspace(0, Lx, nx)
    y = np.linspace(0, Ly, ny)
    X, Y = np.meshgrid(x, y, indexing='ij')

    print("ğŸ”„ ç”Ÿæˆåˆæˆæ•°æ®...")

    # åˆå§‹åŒ–åœº
    fields = np.zeros((nt, nx, ny, 4))  # [time, x, y, channels]

    # æ—¶é—´å‚æ•°
    dt = 0.01
    t_values = np.arange(nt) * dt

    for t_idx, t in enumerate(t_values):
        if t_idx % 1000 == 0:
            print(f"  è¿›åº¦: {t_idx}/{nt} ({100*t_idx/nt:.1f}%)")

        # ç”Ÿæˆæ›´å¤æ‚çš„æ¸©åº¦åœºæ¨¡å¼
        # ä¸»è¦å¯¹æµæ¨¡å¼
        T_base = 0.5 + 0.3 * np.sin(2*np.pi*X/Lx) * np.sin(np.pi*Y/Ly)

        # æ·»åŠ æ—¶é—´æ¼”åŒ–çš„æ‰°åŠ¨
        T_perturbation = (
            0.1 * np.sin(4*np.pi*X/Lx + 0.1*t) * np.cos(2*np.pi*Y/Ly) +
            0.05 * np.cos(6*np.pi*X/Lx - 0.05*t) * np.sin(3*np.pi*Y/Ly) +
            0.02 * np.sin(8*np.pi*X/Lx + 0.03*t) * np.sin(4*np.pi*Y/Ly)
        )

        # å°å°ºåº¦æ¹æµæ‰°åŠ¨
        turbulent_noise = 0.01 * np.random.normal(0, 1, (nx, ny))

        # è¾¹ç•Œå±‚æ•ˆåº”
        boundary_effect = 0.1 * np.exp(-10*Y/Ly) * np.sin(4*np.pi*X/Lx + 0.2*t)

        T = T_base + T_perturbation + turbulent_noise + boundary_effect

        # é€Ÿåº¦åœºï¼ˆç®€åŒ–çš„å¯¹æµæ¨¡å¼ï¼‰
        u = 0.2 * np.sin(2*np.pi*Y/Ly) * np.cos(2*np.pi*X/Lx + 0.1*t)
        v = -0.2 * np.cos(2*np.pi*Y/Ly) * np.sin(2*np.pi*X/Lx + 0.1*t)

        # æ·»åŠ é€Ÿåº¦æ‰°åŠ¨
        u += 0.05 * np.cos(4*np.pi*X/Lx - 0.08*t) * np.sin(2*np.pi*Y/Ly)
        v += 0.05 * np.sin(4*np.pi*X/Lx - 0.08*t) * np.cos(2*np.pi*Y/Ly)

        # å‹åŠ›åœº
        p = 0.01 * np.sin(np.pi*X/Lx) * np.cos(np.pi*Y/Ly) + 0.005 * np.random.normal(0, 1, (nx, ny))

        # å­˜å‚¨åœº
        fields[t_idx, :, :, 0] = T  # æ¸©åº¦
        fields[t_idx, :, :, 1] = p  # å‹åŠ›
        fields[t_idx, :, :, 2] = u  # ué€Ÿåº¦
        fields[t_idx, :, :, 3] = v  # vé€Ÿåº¦

    print("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°HDF5æ–‡ä»¶...")

    # ä¿å­˜åˆ°HDF5æ–‡ä»¶
    with h5py.File(output_file, 'w') as f:
        # ä¿å­˜ä¸»è¦æ•°æ®
        f.create_dataset('fields', data=fields, compression='gzip', compression_opts=6)

        # ä¿å­˜åæ ‡
        f.create_dataset('x', data=x)
        f.create_dataset('y', data=y)
        f.create_dataset('time', data=t_values)

        # ä¿å­˜å‚æ•°
        f.attrs['Ra'] = Ra
        f.attrs['Pr'] = Pr
        f.attrs['Lx'] = Lx
        f.attrs['Ly'] = Ly
        f.attrs['nx'] = nx
        f.attrs['ny'] = ny
        f.attrs['nt'] = nt
        f.attrs['dt'] = dt

    # æ£€æŸ¥æ–‡ä»¶
    file_size = os.path.getsize(output_file) / 1024 / 1024
    print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"  æ–‡ä»¶: {output_file}")
    print(f"  å¤§å°: {file_size:.1f} MB")
    print(f"  å½¢çŠ¶: {fields.shape}")

    return output_file

def estimate_clips(data_file):
    """ä¼°ç®—èƒ½äº§ç”Ÿå¤šå°‘è®­ç»ƒclips"""
    print(f"\nğŸ“Š ä¼°ç®—è®­ç»ƒæ ·æœ¬æ•°:")

    try:
        with h5py.File(data_file, 'r') as f:
            fields_shape = f['fields'].shape
            nt = fields_shape[0]

        print(f"  æ—¶é—´æ­¥æ•°: {nt}")

        # è®¡ç®—clipsæ•°é‡
        # æ¯ä¸ªclipéœ€è¦ clip_length * temporal_downsample = 8 * 4 = 32 timesteps
        clip_length_needed = 8 * 4
        total_clips = max(0, nt - clip_length_needed + 1)

        print(f"  æ¯ä¸ªclipéœ€è¦: {clip_length_needed} timesteps")
        print(f"  å¯äº§ç”Ÿclips: {total_clips}")

        # æŒ‰æ•°æ®é›†åˆ†å‰²
        train_clips = int(total_clips * 0.7)
        val_clips = int(total_clips * 0.15)
        test_clips = total_clips - train_clips - val_clips

        print(f"  æ•°æ®é›†åˆ†å‰²:")
        print(f"    è®­ç»ƒé›†: ~{train_clips} clips")
        print(f"    éªŒè¯é›†: ~{val_clips} clips")
        print(f"    æµ‹è¯•é›†: ~{test_clips} clips")

        if train_clips > 200:
            print("âœ… è®­ç»ƒæ•°æ®å……è¶³!")
        elif train_clips > 100:
            print("âš ï¸  è®­ç»ƒæ•°æ®è¿˜å¯ä»¥")
        else:
            print("âŒ è®­ç»ƒæ•°æ®ä»ç„¶ä¸è¶³")

        return train_clips

    except Exception as e:
        print(f"âŒ æ— æ³•åˆ†ææ–‡ä»¶: {e}")
        return 0

def test_data_loading(data_file):
    """æµ‹è¯•æ–°æ•°æ®çš„åŠ è½½"""
    print(f"\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½:")

    try:
        from cdanet.data import RBDataModule

        data_module = RBDataModule(
            data_dir='./rb_data_numerical',
            spatial_downsample=4,
            temporal_downsample=4,
            clip_length=8,
            batch_size=2,
            normalize=True,
            num_workers=0
        )

        data_module.setup([1e5])

        data_info = data_module.get_dataset_info()
        total_samples = sum(info['num_samples'] for info in data_info.values())

        print(f"  âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")

        for key, info in data_info.items():
            print(f"    {key}: {info['num_samples']} samples")

        if total_samples > 100:
            print("âœ… æ ·æœ¬æ•°é‡è¶³å¤Ÿï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ!")
            return True
        else:
            print("âš ï¸  æ ·æœ¬æ•°é‡ä»ç„¶åå°‘")
            return False

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    print("=" * 60)
    print("å¤§å‹RBæ•°æ®é›†ç”Ÿæˆå™¨")
    print("=" * 60)

    # åˆ›å»ºæ•°æ®é›†
    data_file = create_large_rb_dataset()

    # ä¼°ç®—clipsæ•°é‡
    train_clips = estimate_clips(data_file)

    # æµ‹è¯•æ•°æ®åŠ è½½
    success = test_data_loading(data_file)

    print(f"\n" + "=" * 60)
    print("ğŸ“Š æ€»ç»“:")
    print(f"  æ•°æ®æ–‡ä»¶: {data_file}")
    print(f"  é¢„æœŸè®­ç»ƒclips: {train_clips}")

    if success and train_clips > 100:
        print("ğŸš€ å‡†å¤‡å°±ç»ª! è¿è¡Œè®­ç»ƒ:")
        print("  python train_better.py")
    else:
        print("âš ï¸  å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°")

    print("=" * 60)

if __name__ == '__main__':
    main()