#!/usr/bin/env python3
"""
è¯Šæ–­è®­ç»ƒåçš„æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„å­¦åˆ°äº†ä¸œè¥¿
"""

import torch
import numpy as np
import os
from cdanet.models import CDAnet
from cdanet.data import RBDataModule

def diagnose_model(checkpoint_path):
    """è¯Šæ–­è®­ç»ƒåçš„æ¨¡å‹"""
    print("ğŸ” è¯Šæ–­è®­ç»ƒåçš„æ¨¡å‹")
    print("=" * 50)

    # åŠ è½½æ¨¡å‹
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return

    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    print(f"âœ… åŠ è½½æ¨¡å‹: {checkpoint_path}")

    # æ£€æŸ¥é…ç½®
    if 'config' in checkpoint:
        model_config = checkpoint['config']
        print(f"ğŸ“Š æ¨¡å‹é…ç½®: {model_config}")
    else:
        model_config = {
            'in_channels': 4, 'feature_channels': 128, 'base_channels': 32,
            'mlp_hidden_dims': [256, 256], 'activation': 'relu',
            'coord_dim': 3, 'output_dim': 4
        }

    # åˆ›å»ºæ¨¡å‹
    model = CDAnet(**model_config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"ğŸ“ˆ è®­ç»ƒepoch: {checkpoint.get('epoch', 'Unknown')}")
    print(f"ğŸ“‰ æœ€ä½³éªŒè¯loss: {checkpoint.get('best_val_loss', 'Unknown')}")

    # è®¾ç½®æ•°æ®
    data_module = RBDataModule(
        data_dir='./rb_data_numerical',
        spatial_downsample=4, temporal_downsample=4,
        batch_size=1, normalize=True, num_workers=0
    )
    data_module.setup([1e5])

    # è·å–æ•°æ®ä¿¡æ¯
    data_info = data_module.get_dataset_info()
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    total_samples = 0
    for key, info in data_info.items():
        samples = info['num_samples']
        total_samples += samples
        print(f"  {key}: {samples} æ ·æœ¬")

    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")

    if total_samples < 50:
        print("âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®å¤ªå°‘ï¼Œå¯èƒ½å¯¼è‡´å¿«é€Ÿè®­ç»ƒå’Œå·®çš„æ€§èƒ½")

    # æµ‹è¯•æ¨¡å‹é¢„æµ‹
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹é¢„æµ‹:")
    test_loader = data_module.get_dataloader(1e5, 'test')
    batch = next(iter(test_loader))

    with torch.no_grad():
        predictions = model(batch['low_res'], batch['coords'])

    # åˆ†æé¢„æµ‹
    pred_min = predictions.min().item()
    pred_max = predictions.max().item()
    pred_mean = predictions.mean().item()
    pred_std = predictions.std().item()

    print(f"  é¢„æµ‹èŒƒå›´: [{pred_min:.6f}, {pred_max:.6f}]")
    print(f"  é¢„æµ‹å‡å€¼: {pred_mean:.6f}")
    print(f"  é¢„æµ‹æ ‡å‡†å·®: {pred_std:.6f}")

    # åˆ†æç›®æ ‡å€¼
    target_min = batch['targets'].min().item()
    target_max = batch['targets'].max().item()
    target_mean = batch['targets'].mean().item()
    target_std = batch['targets'].std().item()

    print(f"  ç›®æ ‡èŒƒå›´: [{target_min:.6f}, {target_max:.6f}]")
    print(f"  ç›®æ ‡å‡å€¼: {target_mean:.6f}")
    print(f"  ç›®æ ‡æ ‡å‡†å·®: {target_std:.6f}")

    # åˆ¤æ–­æ¨¡å‹è´¨é‡
    print(f"\nğŸ¯ è¯Šæ–­ç»“æœ:")

    if pred_std < 0.001:
        print("âŒ æ¨¡å‹é¢„æµ‹å‡ ä¹æ˜¯å¸¸æ•°ï¼Œæ²¡æœ‰å­¦åˆ°æ¨¡å¼")
        print("   å»ºè®®: å¢åŠ è®­ç»ƒæ•°æ®ï¼Œé™ä½å­¦ä¹ ç‡ï¼Œå»¶é•¿è®­ç»ƒæ—¶é—´")
    elif pred_std < 0.01:
        print("âš ï¸  æ¨¡å‹é¢„æµ‹å˜åŒ–å¾ˆå°ï¼Œå­¦ä¹ æ•ˆæœå·®")
        print("   å»ºè®®: æ£€æŸ¥æŸå¤±å‡½æ•°æƒé‡ï¼Œå¢åŠ è®­ç»ƒæ—¶é—´")
    else:
        print("âœ… æ¨¡å‹é¢„æµ‹æœ‰åˆç†å˜åŒ–")

    # æ£€æŸ¥é¢„æµ‹å’Œç›®æ ‡çš„ç›¸å…³æ€§
    pred_flat = predictions.flatten().numpy()
    target_flat = batch['targets'].flatten().numpy()

    # è®¡ç®—ç›¸å…³ç³»æ•°
    try:
        correlation = np.corrcoef(pred_flat, target_flat)[0, 1]
        print(f"  é¢„æµ‹-ç›®æ ‡ç›¸å…³ç³»æ•°: {correlation:.4f}")

        if abs(correlation) < 0.1:
            print("âŒ é¢„æµ‹å’Œç›®æ ‡å‡ ä¹æ— ç›¸å…³æ€§")
        elif abs(correlation) < 0.3:
            print("âš ï¸  é¢„æµ‹å’Œç›®æ ‡ç›¸å…³æ€§è¾ƒå¼±")
        else:
            print("âœ… é¢„æµ‹å’Œç›®æ ‡æœ‰ä¸€å®šç›¸å…³æ€§")
    except:
        print("âš ï¸  æ— æ³•è®¡ç®—ç›¸å…³ç³»æ•°")

    # æ£€æŸ¥lossç»„æˆ
    print(f"\nğŸ“ˆ Lossåˆ†æ:")

    # è®¡ç®—å›å½’loss
    mse_loss = torch.nn.functional.mse_loss(predictions, batch['targets'])
    print(f"  MSE Loss: {mse_loss:.6f}")

    # ä¼°ç®—ä¸ºä»€ä¹ˆè®­ç»ƒè¿™ä¹ˆå¿«
    print(f"\nâ±ï¸  è®­ç»ƒé€Ÿåº¦åˆ†æ:")
    samples_per_epoch = sum(info['num_samples'] for info in data_info.values())
    print(f"  æ¯ä¸ªepochæ ·æœ¬æ•°: {samples_per_epoch}")
    print(f"  batch size: 1")
    print(f"  æ¯ä¸ªepochçš„batchæ•°: {samples_per_epoch}")

    if samples_per_epoch < 50:
        print("âŒ æ¯ä¸ªepochæ ·æœ¬å¤ªå°‘ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®­ç»ƒè¿™ä¹ˆå¿«!")
        print("   å»ºè®®:")
        print("   1. ç”Ÿæˆæ›´å¤šè®­ç»ƒæ•°æ® (nx=1024, ny=256, nt=8000+)")
        print("   2. ä½¿ç”¨æ•°æ®å¢å¼º")
        print("   3. å¢åŠ batch size")
        print("   4. ä½¿ç”¨æ›´å¤æ‚çš„æ•°æ®é‡‡æ ·ç­–ç•¥")

    return predictions, batch['targets']

def suggest_improvements():
    """å»ºè®®æ”¹è¿›æ–¹æ¡ˆ"""
    print(f"\nğŸš€ æ”¹è¿›å»ºè®®:")
    print("1. æ•°æ®æ”¹è¿›:")
    print("   - ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡æ•°æ®: nx=1024, ny=256")
    print("   - æ›´å¤šæ—¶é—´æ­¥: nt=10000+ (äº§ç”Ÿæ›´å¤šclips)")
    print("   - ç”Ÿæˆå¤šä¸ªä¸åŒåˆå§‹æ¡ä»¶çš„æ•°æ®é›†")

    print("2. è®­ç»ƒæ”¹è¿›:")
    print("   - é™ä½å­¦ä¹ ç‡åˆ° 1e-4 æˆ–æ›´å°")
    print("   - å¢åŠ PDE lossæƒé‡åˆ° 0.01")
    print("   - å»¶é•¿è®­ç»ƒåˆ° 2000+ epochs")
    print("   - ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")

    print("3. æ¨¡å‹æ”¹è¿›:")
    print("   - å¢åŠ æ¨¡å‹å¤æ‚åº¦ (æ›´å¤šç‰¹å¾é€šé“)")
    print("   - æ·»åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("   - ä½¿ç”¨æ®‹å·®è¿æ¥")

def main():
    checkpoint_path = './checkpoints/improved_model_final.pth'

    if os.path.exists(checkpoint_path):
        diagnose_model(checkpoint_path)
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {checkpoint_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬")

    suggest_improvements()

if __name__ == '__main__':
    main()