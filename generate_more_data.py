#!/usr/bin/env python3
"""
ç”Ÿæˆæ›´å¤šé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®
è§£å†³è®­ç»ƒå¤ªå¿«ã€æ•°æ®å¤ªå°‘çš„é—®é¢˜
"""

import os
import h5py
import numpy as np
from cdanet.data import RBDataModule

def generate_large_dataset():
    """ç”Ÿæˆå¤§å‹é«˜è´¨é‡æ•°æ®é›†"""
    print("ğŸ”„ ç”Ÿæˆå¤§å‹è®­ç»ƒæ•°æ®é›†")
    print("=" * 50)

    data_dir = './rb_data_numerical'
    os.makedirs(data_dir, exist_ok=True)

    # åˆ é™¤æ‰€æœ‰ç°æœ‰çš„æ•°æ®æ–‡ä»¶é˜²æ­¢è¦†ç›–é—®é¢˜
    print("ğŸ—‘ï¸  æ¸…ç†æ—§æ•°æ®æ–‡ä»¶...")
    for file in os.listdir(data_dir):
        if file.endswith('.h5'):
            old_file_path = os.path.join(data_dir, file)
            print(f"  åˆ é™¤: {file}")
            os.remove(old_file_path)

    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = RBDataModule(
        data_dir=data_dir,
        spatial_downsample=4,
        temporal_downsample=4,
        clip_length=8,
        batch_size=1,
        normalize=True
    )

    # ç”Ÿæˆå¤§é‡æ•°æ®çš„æ•°æ®é›†
    datasets = [
        {
            'name': 'rb_data_Ra_1e+05.h5',  # æ›¿æ¢ç°æœ‰æ–‡ä»¶
            'nx': 512,   # ç¨å¾®å¢åŠ åˆ†è¾¨ç‡
            'ny': 128,
            'nt': 8000,  # å¤§å¹…å¢åŠ åˆ°8000æ—¶é—´æ­¥
            'description': 'å¤§é‡è®­ç»ƒæ•°æ®çš„æ•°æ®é›†'
        }
    ]

    generated_files = []

    for i, dataset in enumerate(datasets):
        output_path = os.path.join(data_dir, dataset['name'])

        # æ–‡ä»¶å·²ç»åœ¨ä¸Šé¢æ¸…ç†è¿‡äº†ï¼Œç›´æ¥ç”Ÿæˆ

        print(f"ğŸ”„ ç”Ÿæˆæ•°æ®é›† {i+1}/{len(datasets)}: {dataset['description']}")
        print(f"   åˆ†è¾¨ç‡: {dataset['nx']}x{dataset['ny']}")
        print(f"   æ—¶é—´æ­¥: {dataset['nt']}")

        try:
            # ç”Ÿæˆæ•°æ®
            synthetic_file = data_module.create_synthetic_data(
                output_path=output_path,
                Ra=1e5,
                nx=dataset['nx'],
                ny=dataset['ny'],
                nt=dataset['nt']
            )

            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            if os.path.exists(synthetic_file):
                file_size = os.path.getsize(synthetic_file) / 1024 / 1024
                print(f"   âœ… ç”Ÿæˆå®Œæˆ: {file_size:.1f} MB")

                # æ£€æŸ¥æ•°æ®è´¨é‡
                with h5py.File(synthetic_file, 'r') as f:
                    data_shape = f['fields'].shape
                    print(f"   æ•°æ®å½¢çŠ¶: {data_shape}")

                generated_files.append(synthetic_file)
            else:
                print(f"   âŒ ç”Ÿæˆå¤±è´¥")

        except Exception as e:
            print(f"   âŒ ç”Ÿæˆé”™è¯¯: {e}")
            continue

    return generated_files

def estimate_training_samples(data_files):
    """ä¼°ç®—è®­ç»ƒæ ·æœ¬æ•°é‡"""
    print(f"\nğŸ“Š ä¼°ç®—è®­ç»ƒæ ·æœ¬æ•°é‡:")

    total_clips = 0

    for data_file in data_files:
        if not os.path.exists(data_file):
            continue

        try:
            with h5py.File(data_file, 'r') as f:
                nt = f['fields'].shape[0]  # æ—¶é—´æ­¥æ•°

            # è®¡ç®—å¯èƒ½çš„clipsæ•°é‡
            # æ¯ä¸ªclipé•¿åº¦ä¸º 8 * temporal_downsample = 32 timesteps
            clip_length = 8 * 4  # clip_length * temporal_downsample
            clips_per_file = max(0, nt - clip_length + 1)

            file_size = os.path.getsize(data_file) / 1024 / 1024
            print(f"  {os.path.basename(data_file)}:")
            print(f"    æ—¶é—´æ­¥: {nt}")
            print(f"    å¯èƒ½çš„clips: {clips_per_file}")
            print(f"    æ–‡ä»¶å¤§å°: {file_size:.1f} MB")

            total_clips += clips_per_file

        except Exception as e:
            print(f"  âŒ æ— æ³•è¯»å– {data_file}: {e}")

    print(f"\nğŸ“ˆ æ€»è®¡:")
    print(f"  æ€»clipsæ•°é‡: {total_clips}")
    print(f"  æŒ‰ 80/10/10 åˆ†å‰²:")
    print(f"    è®­ç»ƒé›†: ~{int(total_clips * 0.8)} samples")
    print(f"    éªŒè¯é›†: ~{int(total_clips * 0.1)} samples")
    print(f"    æµ‹è¯•é›†: ~{int(total_clips * 0.1)} samples")

    # ä¼°ç®—è®­ç»ƒæ—¶é—´
    if total_clips > 0:
        train_samples = int(total_clips * 0.8)
        epochs_for_1000_samples = 1000 / max(1, train_samples)
        print(f"\nâ±ï¸  è®­ç»ƒæ—¶é—´ä¼°ç®—:")
        print(f"    æ¯ä¸ªepoch ~{train_samples} ä¸ªæ ·æœ¬")

        if train_samples < 100:
            print(f"    âš ï¸  ä»ç„¶å¤ªå°‘! å»ºè®®ç”Ÿæˆæ›´å¤šæ•°æ®")
        elif train_samples < 500:
            print(f"    âš ï¸  åå°‘ï¼Œä½†å¯ä»¥å°è¯•")
        else:
            print(f"    âœ… æ•°é‡åˆç†")

        if train_samples > 50:
            estimated_time_per_epoch = train_samples * 0.1  # å‡è®¾æ¯ä¸ªæ ·æœ¬0.1ç§’
            print(f"    é¢„è®¡æ¯epochæ—¶é—´: ~{estimated_time_per_epoch:.1f} ç§’")

    return total_clips

def test_data_loading(data_files):
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print(f"\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½:")

    try:
        # åˆ›å»ºæ•°æ®æ¨¡å—
        data_module = RBDataModule(
            data_dir='./rb_data_numerical',
            spatial_downsample=4,
            temporal_downsample=4,
            clip_length=8,
            batch_size=2,  # æµ‹è¯•æ›´å¤§çš„batch size
            normalize=True,
            num_workers=0
        )

        # è®¾ç½®æ•°æ®
        data_module.setup([1e5])

        # è·å–æ•°æ®ä¿¡æ¯
        data_info = data_module.get_dataset_info()
        print("  æ•°æ®é›†ä¿¡æ¯:")
        for key, info in data_info.items():
            print(f"    {key}: {info['num_samples']} æ ·æœ¬, å½¢çŠ¶ {info['high_res_shape']}")

        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        train_loader = data_module.get_dataloader(1e5, 'train')
        test_batch = next(iter(train_loader))

        print(f"  âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"    Batch shapes:")
        print(f"      Low-res: {test_batch['low_res'].shape}")
        print(f"      Targets: {test_batch['targets'].shape}")
        print(f"      Coords: {test_batch['coords'].shape}")

        return True

    except Exception as e:
        print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False

def main():
    print("=" * 60)
    print("é«˜è´¨é‡è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨")
    print("=" * 60)

    # ç”Ÿæˆå¤§å‹æ•°æ®é›†
    data_files = generate_large_dataset()

    if not data_files:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®æ–‡ä»¶")
        return

    # ä¼°ç®—æ ·æœ¬æ•°é‡
    total_clips = estimate_training_samples(data_files)

    # æµ‹è¯•æ•°æ®åŠ è½½
    success = test_data_loading(data_files)

    print(f"\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®ç”Ÿæˆæ€»ç»“:")
    print(f"  ç”Ÿæˆæ–‡ä»¶æ•°: {len(data_files)}")
    print(f"  æ€»è®­ç»ƒclips: {total_clips}")

    if success and total_clips > 100:
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹æ”¹è¿›è®­ç»ƒ!")
        print("å»ºè®®è¿è¡Œ:")
        print("  python simple_improved_train.py")
    elif total_clips < 100:
        print("âš ï¸  æ•°æ®é‡ä»ç„¶ä¸è¶³ï¼Œå»ºè®®:")
        print("  1. å¢åŠ ntå‚æ•° (æ›´å¤šæ—¶é—´æ­¥)")
        print("  2. ç”Ÿæˆæ›´å¤šæ•°æ®é›†")
        print("  3. å‡å°temporal_downsample")
    else:
        print("âŒ æ•°æ®åŠ è½½æœ‰é—®é¢˜ï¼Œéœ€è¦è°ƒè¯•")

    print("=" * 60)

if __name__ == '__main__':
    main()