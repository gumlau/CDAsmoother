#!/usr/bin/env python3
"""
è¯¦ç»†é‡åŒ–è¯„ä¼°è„šæœ¬
æä¾›å…¨é¢çš„æ¨¡å‹æ€§èƒ½åˆ†æå’Œè¯Šæ–­
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os
import sys
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

# ç›´æ¥å¯¼å…¥æ¨¡å‹å’Œæ•°æ®åŠ è½½åŠŸèƒ½
from cdanet.models import CDAnet
from cdanet.data import RB2DataModule

def load_model_from_checkpoint(checkpoint_path, device='cuda'):
    """ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹"""
    print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device)
    model_config = checkpoint['model_config']

    # åˆ›å»ºæ¨¡å‹
    model = CDAnet(
        in_channels=model_config['in_channels'],
        feature_channels=model_config['feature_channels'],
        mlp_hidden_dims=model_config['mlp_hidden_dims'],
        activation=model_config['activation'],
        coord_dim=model_config['coord_dim'],
        output_dim=model_config['output_dim'],
        igres=model_config['igres'],
        unet_nf=model_config['unet_nf'],
        unet_mf=model_config['unet_mf']
    )

    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    return model

def setup_simple_data_module(data_dir, Ra_numbers=[1e5]):
    """ç®€åŒ–çš„æ•°æ®æ¨¡å—è®¾ç½®"""
    print(f"è®¾ç½®æ•°æ®æ¨¡å—: {data_dir}")

    data_module = RB2DataModule(
        data_dir=data_dir,
        Ra_numbers=Ra_numbers,
        batch_size=1,
        normalize=True
    )
    data_module.setup()

    print(f"âœ… æ•°æ®æ¨¡å—è®¾ç½®å®Œæˆ")
    return data_module

def comprehensive_evaluation(model, data_module, device='cuda'):
    """å…¨é¢çš„æ¨¡å‹è¯„ä¼°"""

    print("ğŸ”¬ å¼€å§‹è¯¦ç»†é‡åŒ–è¯„ä¼°...")
    print("=" * 60)

    model.eval()
    all_metrics = {}
    all_predictions = []
    all_targets = []

    # æµ‹è¯•ä¸åŒRaæ•°
    Ra_numbers = [1e5]

    for Ra in Ra_numbers:
        print(f"\nğŸ“Š è¯„ä¼° Ra = {Ra:.0e}")

        test_loader = data_module.get_dataloader(Ra, 'test')

        batch_metrics = {
            'mse': [], 'mae': [], 'r2': [],
            'pred_range': [], 'target_range': [],
            'pred_std': [], 'target_std': [],
            'correlation': []
        }

        with torch.no_grad():
            for i, batch in enumerate(test_loader):
                if i >= 5:  # é™åˆ¶æ‰¹æ¬¡æ•°é¿å…è¿‡é•¿
                    break

                print(f"  å¤„ç†æ‰¹æ¬¡ {i+1}...")

                low_res = batch['low_res'].to(device)
                coords = batch['coords'].to(device)
                targets = batch['targets'].to(device)

                # åˆ†å—æ¨ç†
                batch_size, num_coords, coord_dim = coords.shape
                chunk_size = 4096
                predictions_list = []

                for j in range(0, num_coords, chunk_size):
                    end_idx = min(j + chunk_size, num_coords)
                    coord_chunk = coords[:, j:end_idx, :]

                    pred_chunk = model(low_res, coord_chunk)
                    predictions_list.append(pred_chunk.cpu())

                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                predictions = torch.cat(predictions_list, dim=1).to(device)

                # åå½’ä¸€åŒ–
                if data_module.normalizer is not None:
                    predictions_cpu = predictions.cpu()
                    targets_cpu = targets.cpu()

                    pred_denorm = data_module.normalizer.denormalize(
                        predictions_cpu.view(-1, 4)).view(predictions_cpu.shape)
                    target_denorm = data_module.normalizer.denormalize(
                        targets_cpu.view(-1, 4)).view(targets_cpu.shape)
                else:
                    pred_denorm = predictions.cpu()
                    target_denorm = targets.cpu()

                # è®¡ç®—å„ä¸ªç‰©ç†é‡çš„æŒ‡æ ‡
                for field_idx, field_name in enumerate(['T', 'p', 'u', 'v']):
                    pred_field = pred_denorm[0, :, field_idx].numpy()
                    target_field = target_denorm[0, :, field_idx].numpy()

                    # åŸºç¡€æŒ‡æ ‡
                    mse = mean_squared_error(target_field, pred_field)
                    mae = mean_absolute_error(target_field, pred_field)

                    # ç›¸å…³ç³»æ•°
                    correlation = np.corrcoef(target_field, pred_field)[0, 1]

                    # RÂ²
                    r2 = r2_score(target_field, pred_field)

                    # èŒƒå›´å’Œå˜å¼‚æ€§
                    pred_range = pred_field.max() - pred_field.min()
                    target_range = target_field.max() - target_field.min()

                    batch_metrics['mse'].append(mse)
                    batch_metrics['mae'].append(mae)
                    batch_metrics['r2'].append(r2)
                    batch_metrics['correlation'].append(correlation)
                    batch_metrics['pred_range'].append(pred_range)
                    batch_metrics['target_range'].append(target_range)
                    batch_metrics['pred_std'].append(np.std(pred_field))
                    batch_metrics['target_std'].append(np.std(target_field))

                # ä¿å­˜ç”¨äºè¯¦ç»†åˆ†æ
                all_predictions.append(pred_denorm[0].numpy())
                all_targets.append(target_denorm[0].numpy())

        all_metrics[Ra] = batch_metrics

        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print(f"  ğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
        print(f"    MSE: {np.mean(batch_metrics['mse']):.6f} Â± {np.std(batch_metrics['mse']):.6f}")
        print(f"    MAE: {np.mean(batch_metrics['mae']):.6f} Â± {np.std(batch_metrics['mae']):.6f}")
        print(f"    RÂ²:  {np.mean(batch_metrics['r2']):.4f} Â± {np.std(batch_metrics['r2']):.4f}")
        print(f"    ç›¸å…³ç³»æ•°: {np.mean(batch_metrics['correlation']):.4f} Â± {np.std(batch_metrics['correlation']):.4f}")
        print(f"    é¢„æµ‹èŒƒå›´/çœŸå®èŒƒå›´: {np.mean(batch_metrics['pred_range'])/np.mean(batch_metrics['target_range']):.4f}")
        print(f"    é¢„æµ‹æ ‡å‡†å·®/çœŸå®æ ‡å‡†å·®: {np.mean(batch_metrics['pred_std'])/np.mean(batch_metrics['target_std']):.4f}")

    return all_metrics, all_predictions, all_targets

def create_detailed_plots(all_predictions, all_targets, save_dir='./detailed_analysis'):
    """åˆ›å»ºè¯¦ç»†åˆ†æå›¾è¡¨"""

    os.makedirs(save_dir, exist_ok=True)

    print(f"\nğŸ¨ åˆ›å»ºè¯¦ç»†åˆ†æå›¾è¡¨...")

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_pred = np.concatenate(all_predictions, axis=0)  # [N_total, 4]
    all_true = np.concatenate(all_targets, axis=0)

    field_names = ['Temperature', 'Pressure', 'U-velocity', 'V-velocity']

    # 1. æ•£ç‚¹å›¾ - é¢„æµ‹vsçœŸå®å€¼
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Prediction vs Ground Truth', fontsize=16)

    for i, (ax, field_name) in enumerate(zip(axes.flat, field_names)):
        pred_vals = all_pred[:, i]
        true_vals = all_true[:, i]

        # éšæœºé‡‡æ ·é¿å…è¿‡å¤šç‚¹
        n_points = min(10000, len(pred_vals))
        idx = np.random.choice(len(pred_vals), n_points, replace=False)

        ax.scatter(true_vals[idx], pred_vals[idx], alpha=0.5, s=1)

        # å®Œç¾é¢„æµ‹çº¿
        min_val = min(true_vals.min(), pred_vals.min())
        max_val = max(true_vals.max(), pred_vals.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect')

        ax.set_xlabel(f'True {field_name}')
        ax.set_ylabel(f'Predicted {field_name}')
        ax.set_title(f'{field_name}')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        r2 = r2_score(true_vals, pred_vals)
        correlation = np.corrcoef(true_vals, pred_vals)[0, 1]
        ax.text(0.05, 0.95, f'RÂ²={r2:.3f}\nCorr={correlation:.3f}',
                transform=ax.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.tight_layout()
    plt.savefig(f'{save_dir}/prediction_vs_truth.png', dpi=300, bbox_inches='tight')
    plt.close()

    # 2. è¯¯å·®åˆ†å¸ƒ
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Error Distributions', fontsize=16)

    for i, (ax, field_name) in enumerate(zip(axes.flat, field_names)):
        errors = all_pred[:, i] - all_true[:, i]

        ax.hist(errors, bins=50, alpha=0.7, density=True)
        ax.axvline(0, color='red', linestyle='--', label='Perfect')
        ax.axvline(np.mean(errors), color='green', linestyle='-',
                   label=f'Mean={np.mean(errors):.4f}')

        ax.set_xlabel(f'Error ({field_name})')
        ax.set_ylabel('Density')
        ax.set_title(f'{field_name} Error Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        ax.text(0.05, 0.95, f'Std={np.std(errors):.4f}\nMAE={np.mean(np.abs(errors)):.4f}',
                transform=ax.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.tight_layout()
    plt.savefig(f'{save_dir}/error_distributions.png', dpi=300, bbox_inches='tight')
    plt.close()

    # 3. åœºçš„ç»Ÿè®¡å¯¹æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Field Statistics Comparison', fontsize=16)

    stats_to_plot = ['mean', 'std', 'min', 'max']

    for i, (ax, stat_name) in enumerate(zip(axes.flat, stats_to_plot)):
        true_stats = []
        pred_stats = []

        for j in range(4):  # 4ä¸ªç‰©ç†åœº
            if stat_name == 'mean':
                true_stat = np.mean(all_true[:, j])
                pred_stat = np.mean(all_pred[:, j])
            elif stat_name == 'std':
                true_stat = np.std(all_true[:, j])
                pred_stat = np.std(all_pred[:, j])
            elif stat_name == 'min':
                true_stat = np.min(all_true[:, j])
                pred_stat = np.min(all_pred[:, j])
            else:  # max
                true_stat = np.max(all_true[:, j])
                pred_stat = np.max(all_pred[:, j])

            true_stats.append(true_stat)
            pred_stats.append(pred_stat)

        x = np.arange(len(field_names))
        width = 0.35

        ax.bar(x - width/2, true_stats, width, label='Ground Truth', alpha=0.7)
        ax.bar(x + width/2, pred_stats, width, label='Prediction', alpha=0.7)

        ax.set_xlabel('Field')
        ax.set_ylabel(stat_name.capitalize())
        ax.set_title(f'{stat_name.capitalize()} Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(['T', 'p', 'u', 'v'])
        ax.legend()
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{save_dir}/field_statistics.png', dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ… è¯¦ç»†åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ° {save_dir}/")

def diagnose_model_issues(all_predictions, all_targets):
    """è¯Šæ–­æ¨¡å‹é—®é¢˜"""

    print(f"\nğŸ” æ¨¡å‹é—®é¢˜è¯Šæ–­...")
    print("-" * 40)

    all_pred = np.concatenate(all_predictions, axis=0)
    all_true = np.concatenate(all_targets, axis=0)

    field_names = ['Temperature', 'Pressure', 'U-velocity', 'V-velocity']

    for i, field_name in enumerate(field_names):
        pred_vals = all_pred[:, i]
        true_vals = all_true[:, i]

        print(f"\nğŸ“Š {field_name}:")
        print(f"  çœŸå®å€¼èŒƒå›´: [{true_vals.min():.4f}, {true_vals.max():.4f}]")
        print(f"  é¢„æµ‹å€¼èŒƒå›´: [{pred_vals.min():.4f}, {pred_vals.max():.4f}]")
        print(f"  èŒƒå›´æ¯”ç‡: {(pred_vals.max()-pred_vals.min())/(true_vals.max()-true_vals.min()):.4f}")
        print(f"  å˜å¼‚ç³»æ•°æ¯”: {(np.std(pred_vals)/np.mean(np.abs(pred_vals)))/(np.std(true_vals)/np.mean(np.abs(true_vals))):.4f}")

        # æ£€æŸ¥é—®é¢˜
        issues = []

        if (pred_vals.max() - pred_vals.min()) < 0.1 * (true_vals.max() - true_vals.min()):
            issues.append("âš ï¸  é¢„æµ‹èŒƒå›´è¿‡çª„")

        if np.std(pred_vals) < 0.1 * np.std(true_vals):
            issues.append("âš ï¸  é¢„æµ‹å˜å¼‚æ€§ä¸è¶³")

        if abs(np.mean(pred_vals) - np.mean(true_vals)) > 0.1 * np.std(true_vals):
            issues.append("âš ï¸  é¢„æµ‹å‡å€¼åå·®è¿‡å¤§")

        if np.corrcoef(true_vals, pred_vals)[0, 1] < 0.5:
            issues.append("âš ï¸  ç›¸å…³æ€§ä½")

        if len(issues) == 0:
            issues.append("âœ… çœ‹èµ·æ¥æ­£å¸¸")

        for issue in issues:
            print(f"  {issue}")

def main():
    parser = argparse.ArgumentParser(description='è¯¦ç»†é‡åŒ–è¯„ä¼°')
    parser.add_argument('--checkpoint', required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data_dir', default='./rb_data_final', help='æ•°æ®ç›®å½•')

    args = parser.parse_args()

    print("ğŸš€ CDAnetè¯¦ç»†é‡åŒ–è¯„ä¼°")
    print("=" * 60)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    try:
        model = load_model_from_checkpoint(args.checkpoint, device)
        data_module = setup_simple_data_module(args.data_dir, [1e5])

        # è¯¦ç»†è¯„ä¼°
        all_metrics, all_predictions, all_targets = comprehensive_evaluation(
            model, data_module, device)

        # åˆ›å»ºè¯¦ç»†å›¾è¡¨
        create_detailed_plots(all_predictions, all_targets)

        # è¯Šæ–­é—®é¢˜
        diagnose_model_issues(all_predictions, all_targets)

        print(f"\nğŸ‰ è¯¦ç»†è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: ./detailed_analysis/")

    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()