#!/usr/bin/env python3
"""
æ”¹è¿›çš„è®­ç»ƒè„šæœ¬ï¼Œè§£å†³è®­ç»ƒå¤ªå¿«å’Œæ•ˆæœå·®çš„é—®é¢˜
"""

import os
import torch
from datetime import datetime

from cdanet.models import CDAnet
from cdanet.config import ExperimentConfig
from cdanet.data import RBDataModule
from cdanet.training import CDAnetTrainer
from cdanet.utils import Logger


def main():
    print("=" * 60)
    print("CDAnet æ”¹è¿›è®­ç»ƒ")
    print("=" * 60)

    # åˆ›å»ºé…ç½®
    config = ExperimentConfig()

    config.experiment_name = f"cdanet_better_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    config.description = "æ”¹è¿›çš„è®­ç»ƒé…ç½®ï¼Œæ›´å¤šæ•°æ®å’Œæ›´å¥½å‚æ•°"

    # æ•°æ®é…ç½® - å‡å°‘downsamplingå¢åŠ æ•°æ®é‡
    config.data.data_dir = os.path.abspath('./rb_data_numerical')
    config.data.spatial_downsample = 2  # ä»4å‡å°‘åˆ°2ï¼Œå¢åŠ 4å€ç©ºé—´æ•°æ®
    config.data.temporal_downsample = 2  # ä»4å‡å°‘åˆ°2ï¼Œå¢åŠ 4å€æ—¶é—´æ•°æ®
    config.data.clip_length = 16  # å¢åŠ clipé•¿åº¦
    config.data.Ra_numbers = [1e5]
    config.data.batch_size = 1  # å‡å°batch sizeï¼Œæ›´å¤šæ­¥æ•°
    config.data.num_workers = 0
    config.data.pde_points = 4096  # å¢åŠ PDEç‚¹
    config.data.normalize = True

    # æ¨¡å‹é…ç½® - å’ŒcheckpointåŒ¹é…
    config.model.in_channels = 4
    config.model.feature_channels = 128
    config.model.base_channels = 32
    config.model.mlp_hidden_dims = [256, 256]
    config.model.activation = 'relu'
    config.model.coord_dim = 3
    config.model.output_dim = 4

    # æŸå¤±é…ç½® - å‡ ä¹å…³é—­PDE lossä¸“æ³¨æ•°æ®æ‹Ÿåˆ
    config.loss.lambda_pde = 0.000001  # æå°çš„PDEæƒé‡
    config.loss.regression_norm = 'l2'
    config.loss.pde_norm = 'l2'
    config.loss.Ra = 1e5
    config.loss.Pr = 0.7
    config.loss.Lx = 3.0
    config.loss.Ly = 1.0

    # ä¼˜åŒ–å™¨é…ç½® - æ›´æ¿€è¿›çš„å­¦ä¹ 
    config.optimizer.optimizer_type = 'adam'
    config.optimizer.learning_rate = 0.005  # æ›´é«˜çš„å­¦ä¹ ç‡
    config.optimizer.weight_decay = 1e-6    # æ›´å°çš„weight decay
    config.optimizer.grad_clip_max_norm = 2.0  # æ”¾æ¾æ¢¯åº¦è£å‰ª
    config.optimizer.scheduler_type = 'step'   # ä½¿ç”¨step scheduler
    config.optimizer.step_size = 50           # æ¯50 epochsé™ä½LR
    config.optimizer.gamma = 0.8               # LRè¡°å‡å› å­

    # è®­ç»ƒé…ç½® - æ›´ä»”ç»†çš„è®­ç»ƒ
    config.training.num_epochs = 200  # å‡å°‘epochsï¼Œå…³æ³¨è´¨é‡
    config.training.clips_per_epoch = -1
    config.training.val_interval = 5   # æ›´é¢‘ç¹éªŒè¯
    config.training.checkpoint_interval = 25
    config.training.save_best = True
    config.training.early_stopping = True
    config.training.patience = 50      # å‡å°‘patience
    config.training.min_delta = 1e-4   # æ›´å¤§çš„min_deltaè¦æ±‚çœŸæ­£çš„æ”¹è¿›
    config.training.use_amp = False
    config.training.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    config.training.log_interval = 1
    config.training.output_dir = './outputs'
    config.training.checkpoint_dir = './checkpoints'
    config.training.log_dir = './logs'

    print(f"å®éªŒåç§°: {config.experiment_name}")
    print(f"å­¦ä¹ ç‡: {config.optimizer.learning_rate}")
    print(f"PDEæƒé‡: {config.loss.lambda_pde}")
    print(f"è®¾å¤‡: {config.training.device}")

    # å¼ºåˆ¶é‡æ–°ç”Ÿæˆå¹²å‡€çš„æ•°æ®
    print("ğŸ—‘ï¸  åˆ é™¤æœ‰é—®é¢˜çš„æ—§æ•°æ®ï¼Œé‡æ–°ç”Ÿæˆ...")

    data_file = os.path.join(config.data.data_dir, f'rb_data_Ra_{config.data.Ra_numbers[0]:.0e}.h5')
    if os.path.exists(data_file):
        os.remove(data_file)
        print("  åˆ é™¤æ—§æ•°æ®æ–‡ä»¶")

    # ä½¿ç”¨æ”¹è¿›çš„RB simulationç”Ÿæˆæ›´å¤šçœŸå®æ•°æ®
    print("ğŸ”„ è¿è¡Œæ”¹è¿›çš„RB simulation...")
    import subprocess
    result = subprocess.run([
        'python3', 'rb_simulation.py',
        '--Ra', '1e5',
        '--n_runs', '20',  # 20ä¸ªruns
        '--nx', '512',     # æ›´é«˜åˆ†è¾¨ç‡
        '--ny', '128',
        '--nt', '3000'     # æ›´å¤šæ—¶é—´æ­¥
    ], capture_output=True, text=True, cwd='.')

    if result.returncode == 0:
        print("âœ… RB simulationå®Œæˆ")
        # è½¬æ¢æ•°æ®
        conv_result = subprocess.run(['python3', 'convert_rb_data.py'], capture_output=True, text=True)
        if conv_result.returncode == 0:
            print("âœ… æ•°æ®è½¬æ¢å®Œæˆ")
        else:
            print("âš ï¸  æ•°æ®è½¬æ¢æœ‰é—®é¢˜ï¼Œä½†ç»§ç»­è®­ç»ƒ")
    else:
        print(f"âŒ RB simulationå¤±è´¥: {result.stderr}")
        print("ç¨‹åºç»ˆæ­¢")
        return

    print("=" * 60)

    # è®¾ç½®æ•°æ®
    data_module = RBDataModule(
        data_dir=config.data.data_dir,
        spatial_downsample=config.data.spatial_downsample,
        temporal_downsample=config.data.temporal_downsample,
        clip_length=config.data.clip_length,
        batch_size=config.data.batch_size,
        num_workers=config.data.num_workers,
        pde_points=config.data.pde_points,
        normalize=config.data.normalize
    )

    data_module.setup(config.data.Ra_numbers)

    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    data_info = data_module.get_dataset_info()
    total_samples = sum(info['num_samples'] for info in data_info.values())
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")

    for key, info in data_info.items():
        print(f"  {key}: {info['num_samples']} samples")

    if total_samples < 100:
        print("âš ï¸  æ ·æœ¬æ•°ä»ç„¶å¤ªå°‘ï¼Œè®­ç»ƒå¯èƒ½å¾ˆå¿«å®Œæˆ")

    # åˆ›å»ºæ¨¡å‹
    model = CDAnet(
        in_channels=config.model.in_channels,
        feature_channels=config.model.feature_channels,
        base_channels=config.model.base_channels,
        mlp_hidden_dims=config.model.mlp_hidden_dims,
        activation=config.model.activation,
        coord_dim=config.model.coord_dim,
        output_dim=config.model.output_dim
    )

    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params:,}")

    # è®¾ç½®æ—¥å¿—
    logger = Logger(
        log_dir=config.training.log_dir,
        experiment_name=config.experiment_name,
        use_tensorboard=True,
        use_wandb=False,
        config=config.to_dict()
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CDAnetTrainer(
        config=config,
        model=model,
        data_module=data_module,
        logger=logger
    )

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆ")

        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"  æœ€ç»ˆepoch: {trainer.current_epoch}")
        print(f"  æœ€ä½³éªŒè¯loss: {trainer.best_val_loss:.6f}")

    except KeyboardInterrupt:
        print("â¹ï¸  ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise
    finally:
        logger.close()

    # ä¿å­˜æ¨¡å‹
    final_checkpoint = os.path.join(config.training.checkpoint_dir, 'better_model_final.pth')
    try:
        trainer._save_checkpoint('better_model_final.pth', trainer.current_epoch, {})
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {final_checkpoint}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜å¤±è´¥: {e}")

    print("=" * 60)
    print("è®­ç»ƒå®Œæˆ! è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç»“æœ:")
    print(f"python visualize_results.py --checkpoint {final_checkpoint}")
    print("=" * 60)


if __name__ == '__main__':
    main()