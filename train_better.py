#!/usr/bin/env python3
"""
æ”¹è¿›çš„è®­ç»ƒè„šæœ¬ï¼Œè§£å†³è®­ç»ƒå¤ªå¿«å’Œæ•ˆæœå·®çš„é—®é¢˜
"""

import os
import torch
from datetime import datetime

from cdanet.models import CDAnet
from cdanet.config import ExperimentConfig
from cdanet.data import RBDataModule
from cdanet.training import CDAnetTrainer
from cdanet.utils import Logger


def main():
    print("=" * 60)
    print("CDAnet æ”¹è¿›è®­ç»ƒ")
    print("=" * 60)

    # åˆ›å»ºé…ç½®
    config = ExperimentConfig()

    config.experiment_name = f"cdanet_better_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    config.description = "æ”¹è¿›çš„è®­ç»ƒé…ç½®ï¼Œæ›´å¤šæ•°æ®å’Œæ›´å¥½å‚æ•°"

    # æ•°æ®é…ç½®
    config.data.data_dir = os.path.abspath('./rb_data_numerical')
    config.data.spatial_downsample = 4
    config.data.temporal_downsample = 4
    config.data.clip_length = 8
    config.data.Ra_numbers = [1e5]
    config.data.batch_size = 4  # ç¨å¤§çš„batch size
    config.data.num_workers = 0
    config.data.pde_points = 2048
    config.data.normalize = True

    # æ¨¡å‹é…ç½® - å’ŒcheckpointåŒ¹é…
    config.model.in_channels = 4
    config.model.feature_channels = 128
    config.model.base_channels = 32
    config.model.mlp_hidden_dims = [256, 256]
    config.model.activation = 'relu'
    config.model.coord_dim = 3
    config.model.output_dim = 4

    # æŸå¤±é…ç½®
    config.loss.lambda_pde = 0.001
    config.loss.regression_norm = 'l2'
    config.loss.pde_norm = 'l2'
    config.loss.Ra = 1e5
    config.loss.Pr = 0.7
    config.loss.Lx = 3.0
    config.loss.Ly = 1.0

    # ä¼˜åŒ–å™¨é…ç½®
    config.optimizer.optimizer_type = 'adam'
    config.optimizer.learning_rate = 0.0005  # ä¿å®ˆçš„å­¦ä¹ ç‡
    config.optimizer.weight_decay = 1e-5
    config.optimizer.grad_clip_max_norm = 1.0
    config.optimizer.scheduler_type = 'plateau'
    config.optimizer.patience = 30
    config.optimizer.factor = 0.5
    config.optimizer.min_lr = 1e-7

    # è®­ç»ƒé…ç½®
    config.training.num_epochs = 500
    config.training.clips_per_epoch = -1
    config.training.val_interval = 10
    config.training.checkpoint_interval = 50
    config.training.save_best = True
    config.training.early_stopping = True
    config.training.patience = 100
    config.training.min_delta = 1e-6
    config.training.use_amp = False
    config.training.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    config.training.log_interval = 1
    config.training.output_dir = './outputs'
    config.training.checkpoint_dir = './checkpoints'
    config.training.log_dir = './logs'

    print(f"å®éªŒåç§°: {config.experiment_name}")
    print(f"å­¦ä¹ ç‡: {config.optimizer.learning_rate}")
    print(f"PDEæƒé‡: {config.loss.lambda_pde}")
    print(f"è®¾å¤‡: {config.training.device}")

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶å¤§å°
    data_file = os.path.join(config.data.data_dir, f'rb_data_Ra_{config.data.Ra_numbers[0]:.0e}.h5')
    if os.path.exists(data_file):
        file_size = os.path.getsize(data_file) / 1024 / 1024
        print(f"æ•°æ®æ–‡ä»¶: {file_size:.1f} MB")

        if file_size < 10:
            print("âš ï¸  æ•°æ®æ–‡ä»¶å¤ªå°ï¼Œå»ºè®®å…ˆè¿è¡Œ generate_more_data.py")
    else:
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ generate_more_data.py")
        return

    print("=" * 60)

    # è®¾ç½®æ•°æ®
    data_module = RBDataModule(
        data_dir=config.data.data_dir,
        spatial_downsample=config.data.spatial_downsample,
        temporal_downsample=config.data.temporal_downsample,
        clip_length=config.data.clip_length,
        batch_size=config.data.batch_size,
        num_workers=config.data.num_workers,
        pde_points=config.data.pde_points,
        normalize=config.data.normalize
    )

    data_module.setup(config.data.Ra_numbers)

    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    data_info = data_module.get_dataset_info()
    total_samples = sum(info['num_samples'] for info in data_info.values())
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")

    for key, info in data_info.items():
        print(f"  {key}: {info['num_samples']} samples")

    if total_samples < 100:
        print("âš ï¸  æ ·æœ¬æ•°ä»ç„¶å¤ªå°‘ï¼Œè®­ç»ƒå¯èƒ½å¾ˆå¿«å®Œæˆ")

    # åˆ›å»ºæ¨¡å‹
    model = CDAnet(
        in_channels=config.model.in_channels,
        feature_channels=config.model.feature_channels,
        base_channels=config.model.base_channels,
        mlp_hidden_dims=config.model.mlp_hidden_dims,
        activation=config.model.activation,
        coord_dim=config.model.coord_dim,
        output_dim=config.model.output_dim
    )

    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params:,}")

    # è®¾ç½®æ—¥å¿—
    logger = Logger(
        log_dir=config.training.log_dir,
        experiment_name=config.experiment_name,
        use_tensorboard=True,
        use_wandb=False,
        config=config.to_dict()
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CDAnetTrainer(
        config=config,
        model=model,
        data_module=data_module,
        logger=logger
    )

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆ")

        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"  æœ€ç»ˆepoch: {trainer.current_epoch}")
        print(f"  æœ€ä½³éªŒè¯loss: {trainer.best_val_loss:.6f}")

    except KeyboardInterrupt:
        print("â¹ï¸  ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise
    finally:
        logger.close()

    # ä¿å­˜æ¨¡å‹
    final_checkpoint = os.path.join(config.training.checkpoint_dir, 'better_model_final.pth')
    try:
        trainer._save_checkpoint('better_model_final.pth', trainer.current_epoch, {})
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {final_checkpoint}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜å¤±è´¥: {e}")

    print("=" * 60)
    print("è®­ç»ƒå®Œæˆ! è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç»“æœ:")
    print(f"python visualize_results.py --checkpoint {final_checkpoint}")
    print("=" * 60)


if __name__ == '__main__':
    main()